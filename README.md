# Optimizing an ML Pipeline in Azure

1. [ Overview. ](#overview)
2. [ Summary. ](#summary)
3. [ Scikit-learn Pipeline. ](#sci)
4. [ AutoML. ](#auto)
5. [ Pipeline Comparison. ](#comp)
6. [ Future Work. ](#future)


<a name="overview"></a>
## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

<a name="summary"></a>
## Summary
The dataset is called **bankmarketing**, it is a database of customers of a banck that contains some demographic data: "age", "marital_status", "education", "whether they have a loan or not", etc. We want to predict the column called "y", which indicates (probably) whether they responded to a marketing campaign or not.
The best performing model was VotingEnsamble, with an accuracy of 0.91.

<a name="sci"></a>
## Scikit-learn Pipeline
The data was downloaded from the following [URL](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv). All the features were converted to numerical values and then HyperDrive was used to finde the best parameters for a LogisticRegression model.
The parameter "C" (inverse of regularization strength) was chosen randomly between 0.1 and 1. 
Hyperdrive found that the optimal C is 0.97, getting an accuracy of 0.908.

In general, a lower value of "C" penalizes higher values of the parameters of the model (hence avoiding overfitting). However, a very low value of "C" may be counterproductive and the model won't learn properly. So it is a good idea to let Hyperdrive to find the right value.
The use of RandomParameterSampler is sometimes preferable over GridParameterSampler if the computational cost of examining the whole hyperparameter space is too high.

We choose a BanditPolicy with slack_factor=0.1 and evaluation_interval=2. This means that when the accuracy of a run (after 2 iterations) is 10% lower than the best one that run is cancelled. 
We wanted a tight policy, hence we chose BanditPolicy over MedianStoppingPolicy, since the latter only cancels the runs whose metric are below the average of the previous ones.

<a name="auto"></a>
## AutoML
The best model, chosen by AutoML, was VotingEnsamble with an accuracy of 0.918. The hyperparameters generated by AutoML are the models used in VotingEnsamble (LightGBM, RandomForest, XGBoost, etc) with their respective weights.

<a name="comp"></a>
## Pipeline comparison
As mentioned before, the model "VotingEnsamble", found by AutoML, was superior to "LogisticRegression", even with its hyperparameters optimized with HyperDrive. In the first case, the accuracy was 0.917, while in the second case was of 0.908. Being two different models, their architecture is not comparable. 
Since AutoML tries several model architectures at once and choose the one with the best performance, it is natural to expect that AutoML will get better results than HyperDrive, which only considers the unique model introduced by the user.

<a name="future"></a>
## Future work
- One of the nicest features of AutoML is that it alerts the user when the dataset is imbalanced, like in this case. The first step to improve the results will be to deal with the imbalance problem (using some augmentation technique, for example).
- The use of "accuracy" as the performance metric should be look up in detail, maybe it would be better to use other metrics like "precision" or "recall" to measure the model as well. The previous changes would improve the pipelines giving more precise information about the performance of the models.
- In order to improve the model itself, one possibility would be to perform hyperparameter tuning on that specific model (VotingClassifier), with a greater set of parameters, with the help of BayesianSampling.


